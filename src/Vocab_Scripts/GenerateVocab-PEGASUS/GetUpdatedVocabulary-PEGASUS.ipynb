{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f41abb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import sentencepiece_model_pb2 as model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer\n",
    "\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "tokenizer.save_pretrained(\"./PEGASUS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81f81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace Dataset with the name of the dataset you are using\n",
    "#replace /path/to/allVocabFilesforDataset/ with the path to the directory containing all the vocab files for the dataset\n",
    "\n",
    "os.mkdir('./Dataset-PEGASUS')\n",
    "\n",
    "for fname in glob.glob('/path/to/allVocabFilesforDataset/*'):\n",
    "    m = model.ModelProto()\n",
    "    m.ParseFromString(open(\"./PEGASUS/spiece.model\", \"rb\").read())\n",
    "    dir_name = f'./Dataset-PEGASUS/{fname.split(\"/\")[-1][:-4]}'\n",
    "    \n",
    "    print(dir_name)\n",
    "    if not os.path.exists(dir_name): os.mkdir(dir_name)\n",
    "    \n",
    "    special_tokens = open(fname, \"r\").read().split(\"\\n\")[:-1]\n",
    "    for idx,row in enumerate(special_tokens):\n",
    "        token, score = row.split()\n",
    "        new_token = model.ModelProto().SentencePiece()\n",
    "        new_token.piece = token\n",
    "        new_token.score = float(score)\n",
    "        m.pieces.append(new_token)\n",
    "\n",
    "    with open(f'{dir_name}/spiece.model', 'wb') as f:\n",
    "        f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23962dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#replace dataset with the name of the dataset you are using\n",
    "df = pd.read_csv('/path/to/TokenDistCSVinGenerateCandidateWordsforDataser/')\n",
    "terms_EBM = df[df['TokenFrom']=='dataset']['Token'].to_list()\n",
    "freq_ebm = df[df['TokenFrom']=='dataset']['Frequency'].to_list()\n",
    "split_bart = df[df['TokenFrom'] == 'dataset']['SplitSize'].to_list()\n",
    "\n",
    "sum_num = 0.\n",
    "sum_den = 0.\n",
    "for idx,term in enumerate(terms_EBM):\n",
    "    sum_num += split_bart[idx]*freq_ebm[idx]\n",
    "    sum_den += freq_ebm[idx]\n",
    "\n",
    "print(sum_num/sum_den)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import PegasusTokenizer\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "#Replace Dataset with the name of the dataset you are using\n",
    "dict_scores = defaultdict(lambda : defaultdict(dict))\n",
    "for fname in sorted(glob.glob(f'./Dataset-PEGASUS/*'),key = lambda x: [int(x.split('/')[-1].split('_')[-3][:-1]),float(x.split('/')[-1].split('_')[-2])]):\n",
    "    domain_tok = PegasusTokenizer.from_pretrained(fname)\n",
    "    sum_num = 0.\n",
    "    sum_den = 0.\n",
    "    \n",
    "    for idx,term in enumerate(terms_EBM):\n",
    "        sum_num += min(len(domain_tok.tokenize(term)),len(domain_tok.tokenize(' '+term)))*freq_ebm[idx]\n",
    "        sum_den += freq_ebm[idx]\n",
    "\n",
    "    print(fname, sum_num/sum_den, len(domain_tok))\n",
    "    key = fname.split('/')[-1].split('_')\n",
    "    dict_scores[key[-3]][key[-2]] = [round(sum_num/sum_den,2),len(domain_tok)]\n",
    "\n",
    "for k1 in dict_scores:\n",
    "    if k1 == '0K': continue\n",
    "    print('data',end='\\t')\n",
    "    for k2 in dict_scores[k1]:\n",
    "        print(k2,end='\\t')\n",
    "    print()\n",
    "    break\n",
    "\n",
    "for k1 in dict_scores:\n",
    "    print(k1,end='\\t')\n",
    "    for k2 in dict_scores[k1]:\n",
    "        print(f'{dict_scores[k1][k2][0]}/{dict_scores[k1][k2][1]}',end='\\t')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77d351",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProbSum[2.56] -- 15K-0.25\n",
    "da  0.0\t        0.25\t    0.5\t        0.75\t    1.0\t        2.0\t        3.0\t        4.0\t        5.0\t        6.0\t        7.0\t        8.0\t        9.0\t        10.0\t\n",
    "0K\t2.46/96167\t\n",
    "5K\t2.55/96105\t2.55/96105\t2.55/96106\t2.55/96106\t2.55/96107\t2.55/96109\t2.55/96111\t2.55/96113\t2.55/96115\t2.55/96117\t2.55/96119\t2.55/96121\t2.55/96123\t2.55/96125\t\n",
    "10K\t2.53/96112\t2.53/96114\t2.53/96116\t2.53/96118\t2.53/96121\t2.53/96130\t2.53/96139\t2.53/96148\t2.53/96157\t2.53/96166\t2.53/96175\t2.53/96184\t2.53/96193\t2.53/96202\t\n",
    "15K\t2.52/96116\t2.52/96119\t2.52/96122\t2.52/96125\t2.52/96129\t2.52/96142\t2.52/96155\t2.52/96168\t2.52/96181\t2.52/96194\t2.52/96207\t2.52/96220\t2.52/96233\t2.52/96246\t\n",
    "20K\t2.51/96121\t2.51/96125\t2.51/96130\t2.51/96134\t2.51/96139\t2.51/96157\t2.51/96175\t2.51/96193\t2.51/96211\t2.51/96229\t2.51/96247\t2.51/96265\t2.5/96283\t2.5/96301\t\n",
    "25K\t2.51/96123\t2.51/96128\t2.51/96133\t2.51/96138\t2.51/96143\t2.51/96163\t2.51/96183\t2.51/96203\t2.5/96223\t2.5/96243\t2.5/96263\t2.5/96283\t2.5/96303\t2.5/96323\t\n",
    "30K\t2.51/96123\t2.51/96128\t2.51/96133\t2.51/96138\t2.51/96143\t2.51/96163\t2.51/96183\t2.51/96203\t2.5/96223\t2.5/96243\t2.5/96263\t2.5/96283\t2.5/96303\t2.5/96323\t\n",
    "35K\t2.51/96125\t2.51/96130\t2.51/96136\t2.51/96141\t2.51/96147\t2.51/96169\t2.51/96191\t2.51/96213\t2.5/96235\t2.5/96257\t2.5/96279\t2.5/96301\t2.5/96323\t2.49/96345\t\n",
    "40K\t2.51/96127\t2.51/96133\t2.51/96139\t2.51/96145\t2.51/96151\t2.51/96175\t2.51/96199\t2.5/96223\t2.5/96247\t2.5/96271\t2.5/96295\t2.5/96319\t2.49/96343\t2.49/96367\t\n",
    "45K\t2.49/96130\t2.49/96136\t2.49/96143\t2.49/96150\t2.49/96157\t2.49/96184\t2.49/96211\t2.49/96238\t2.49/96265\t2.49/96292\t2.48/96319\t2.48/96346\t2.48/96373\t2.48/96400\t\n",
    "50K\t2.49/96130\t2.49/96136\t2.49/96143\t2.49/96150\t2.49/96157\t2.49/96184\t2.49/96211\t2.49/96238\t2.49/96265\t2.49/96292\t2.48/96319\t2.48/96346\t2.48/96373\t2.48/96400\t\n",
    "55K\t2.49/96132\t2.49/96139\t2.49/96146\t2.49/96153\t2.49/96161\t2.49/96190\t2.49/96219\t2.49/96248\t2.49/96277\t2.48/96306\t2.48/96335\t2.48/96364\t2.48/96393\t2.48/96422\t\n",
    "60K\t2.49/96133\t2.49/96140\t2.49/96148\t2.49/96155\t2.49/96163\t2.49/96193\t2.49/96223\t2.49/96253\t2.49/96283\t2.48/96313\t2.48/96343\t2.48/96373\t2.48/96403\t2.48/96433\t\n",
    "\n",
    "\n",
    "MIMIC [2.98] -- 20K-6\n",
    "da\t0.0\t        0.25\t    0.5\t        0.75\t    1.0\t        2.0\t        3.0\t        4.0\t        5.0\t        6.0\t        7.0\t        8.0\t        9.0\t        10.0\t\n",
    "0K\t2.27/96491\t\n",
    "5K\t2.61/96151\t2.61/96163\t2.61/96175\t2.61/96187\t2.61/96199\t2.6/96247\t2.6/96295\t2.6/96343\t2.6/96391\t2.6/96439\t2.6/96487\t2.59/96535\t2.59/96583\t2.59/96631\t\n",
    "10K\t2.59/96177\t2.59/96195\t2.59/96214\t2.59/96232\t2.59/96251\t2.58/96325\t2.58/96399\t2.58/96473\t2.57/96547\t2.57/96621\t2.57/96695\t2.57/96769\t2.38/96843\t2.37/96917\t\n",
    "15K\t2.58/96194\t2.58/96216\t2.58/96239\t2.58/96262\t2.57/96285\t2.57/96376\t2.57/96467\t2.57/96558\t2.56/96649\t2.56/96740\t2.37/96831\t2.37/96922\t2.37/97013\t2.36/97104\t\n",
    "20K\t2.45/96212\t2.45/96239\t2.45/96266\t2.45/96293\t2.45/96321\t2.44/96430\t2.44/96539\t2.44/96648\t2.44/96757\t2.25/96866\t2.24/96975\t2.24/97084\t2.24/97193\t2.24/97302\t\n",
    "25K\t2.45/96221\t2.45/96250\t2.45/96280\t2.44/96309\t2.44/96339\t2.44/96457\t2.44/96575\t2.44/96693\t2.44/96811\t2.24/96929\t2.24/97047\t2.24/97165\t2.24/97283\t2.23/97401\t\n",
    "30K\t2.46/96232\t2.45/96264\t2.45/96296\t2.45/96328\t2.45/96361\t2.44/96490\t2.44/96619\t2.44/96748\t2.25/96877\t2.24/97006\t2.24/97135\t2.24/97264\t2.24/97393\t2.24/97522\t\n",
    "35K\t2.46/96241\t2.46/96275\t2.46/96310\t2.45/96344\t2.45/96379\t2.45/96517\t2.45/96655\t2.44/96793\t2.25/96931\t2.25/97069\t2.25/97207\t2.24/97345\t2.24/97483\t2.24/97621\t\n",
    "40K\t2.46/96250\t2.46/96286\t2.46/96323\t2.45/96360\t2.45/96397\t2.45/96544\t2.44/96691\t2.44/96838\t2.25/96985\t2.25/97132\t2.25/97279\t2.24/97426\t2.24/97573\t2.23/97720\t\n",
    "45K\t2.45/96259\t2.45/96298\t2.45/96337\t2.45/96376\t2.45/96415\t2.44/96571\t2.44/96727\t2.3/96883\t2.24/97039\t2.24/97195\t2.24/97351\t2.24/97507\t2.23/97663\t2.23/97819\t\n",
    "50K\t2.45/96263\t2.45/96303\t2.45/96343\t2.45/96383\t2.44/96423\t2.44/96583\t2.44/96743\t2.25/96903\t2.24/97063\t2.24/97223\t2.24/97383\t2.24/97543\t2.23/97703\t2.23/97863\t\n",
    "55K\t2.45/96270\t2.45/96311\t2.45/96353\t2.44/96395\t2.44/96437\t2.44/96604\t2.44/96771\t2.24/96938\t2.24/97105\t2.24/97272\t2.24/97439\t2.24/97606\t2.23/97773\t2.23/97940\t\n",
    "60K\t2.45/96278\t2.45/96321\t2.45/96365\t2.44/96409\t2.44/96453\t2.44/96628\t2.43/96803\t2.24/96978\t2.24/97153\t2.24/97328\t2.23/97503\t2.23/97678\t2.23/97853\t2.23/98028\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
